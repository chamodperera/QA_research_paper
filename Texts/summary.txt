Deep Residual Learning for Image Recognition is a paper that introduces a new framework for training deeper neural networks. The paper presents empirical evidence that these residual networks are easier to optimize and can gain accuracy from increased depth. On the ImageNet dataset, the paper evaluates residual nets with a depth of up to 152 layers, 8 layers deeper than VGG nets, and achieves 3.57% error on the ImageNet testset. The paper also presents analysis on CIFAR-10 with 100 and 1000 layers, and obtains a 28% relative improvement on the COCO object detection dataset. The paper concludes that network depth is of crucial importance for many visual recognition tasks, and that deep residual nets are foundations of submissions to ILSVRC & COCO 2015 competitions, where the paper won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation. This paper presents a method of using shortcut connections in feedforward neural networks to optimize a residual mapping of the desired underlying mapping, H(x). This is hypothesized to be easier than optimizing the original, unreferenced mapping. Experiments on the ImageNet dataset show that deep residual nets are easy to optimize and can easily enjoy accuracy gains from increased depth, producing results substantially better than previous networks. The residual learning principle is also shown to be generic and applicable to other vision and non-vision problems. It is also shown that encoding residual vectors is more effective than encoding original vectors in vector quantization and solving Partial Differential Equations (PDEs). Practices and theories that lead to shortcut connections have been studied for a long time, and this paper presents a parameter-free identity shortcut that is never closed and always passes information through. Deep Residual Learning is a reformulation of the hypothesis that multiple nonlinear layers can asymptotically approximate complicated functions. Rather than expecting stacked layers to approximate the underlying mapping, the reformulation explicitly lets these layers approximate a residual function, F(x) :=H(x) x. This reformulation is motivated by the counterintuitive phenomena about the degradation problem, which suggests that the solvers might have difficulty in approximating identity mappings by multiple nonlinear layers. To address this problem, a building block is introduced which consists of a residual mapping to be learned, F(x;fWig), and a shortcut connection which performs an element-wise addition of the input and output vectors of the layers considered. This building block introduces neither extra parameter nor computation complexity. Two models for ImageNet are described, which are mainly inspired by the philosophy of VGG nets. This paper presents a convolutional neural network (CNN) architecture with 34 layers. The convolutional layers mostly have 3x3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is halved, the number of filters is doubled so as to preserve the time complexity per layer. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 and the model has fewer filters and lower complexity than VGG nets. The 34-layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs). This architecture is a great example of how to design a CNN with fewer parameters and lower complexity while still achieving good performance. This paper evaluates the performance of plain and residual networks on the ImageNet 2012 classification dataset. The plain network consists of 34 parameter layers and the residual network consists of 34 parameter layers with shortcut connections. The plain network is compared to the VGG-19 model as a reference. The results show that the deeper 34-layer plain network has higher validation error than the shallower 18-layer plain network. The training and validation errors of the two networks are compared during the training procedure. The paper also discusses implementation details such as image resizing, color augmentation, batch normalization, weight initialization, and testing methods. The results of the evaluation are reported on the 50k validation images and the 100k test images. This paper examines the degradation problem in deep plain networks, where the 4-layer name output size 18-layer 34-layer 50-layer 101-layer 152-layer conv1 112 112 7 7, 64, stride 2 conv2 x 56 563 3 max pool, stride 2 3 3, 64 3 3, 64 2 3 3, 64 3 3, 64 32 41 1, 64 3 3, 64 1 1, 2563 5 32 41 1, 64 3 3, 64 1 1, 2563 5 32 41 1, 64 3 3, 64 1 1, 2563 5 3 conv3 x 28 28 3 3, 128 3 3, 128 2 3 3, 128 3 3, 128 42 41 1, 128 3 3, 128 1 1, 5123 5 42 41 1, 128 3 3, 128 1 1, 5123 5 42 41 1, 128 3 3, 128 1 1, 5123 5 8 conv4 x 14 14 3 3, 256 3 3, 256 2 3 3, 256 3 3, 256 62 41 1, 256 3 3, 256 1 1, 10243 5 62 41 1, 256 3 3, 256 1 1, 10243 5 232 41 1, 256 3 3, 256 1 1, 10243 5 36 conv5 x 7 7 3 3, 512 3 3, 512 2 3 3, 512 3 3, 512 32 41 1, 512 3 3, 512 1 1, 20483 5 32 41 1, 512 3 3, 512 1 1, 20483 5 32 41 1, 512 3 3, 512 1 1, 20483 5 3 1 1 average pool, 1000-d fc, softmax FLOPs 1.8 1093.6 1093.8 1097.6 10911.3 109 is observed. The plain networks of 18 and 34 layers are trained and tested on ImageNet validation, and the results show that the 34-layer plain net has higher training error throughout the whole training procedure. It is argued that this optimization difficulty is unlikely to be caused by vanishing gradients, and is instead likely due to exponentially low convergence rates. Further research is needed to understand the cause of this optimization difficulty. This article evaluates the performance of 18-layer and 34-layer residual networks (ResNets) compared to plain networks. The baseline architectures are the same as the plain networks, except that a shortcut connection is added to each pair of 3x3 filters. The results show that the 34-layer ResNet is better than the 18-layer ResNet, with a lower training error and better generalizability to the validation data. This indicates that the degradation problem is addressed in this setting and accuracy gains from increased depth are obtained. Additionally, the 18-layer plain/residual nets are comparably accurate, but the 18-layer ResNet converges faster. The article also investigates projection shortcuts, which are parameter-free and help with training. The results show that projection shortcuts are effective for increasing dimensions and improving accuracy. This paper examines the effects of different options for addressing the degradation problem in plain networks. Table 3 shows that all three options are considerably better than the plain counterpart. Option C is marginally better than B, and this is attributed to the extra parameters introduced by the projection shortcuts. However, the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. Therefore, identity shortcuts are used in the rest of the paper to reduce memory/time complexity and model sizes. The paper then describes deeper networks for ImageNet, using a bottleneck design with a stack of 3 layers instead of 2. This is done to reduce training time. The parameter-free identity shortcuts are particularly important for the bottleneck architectures. The paper then introduces 50-layer, 101-layer, and 152-layer ResNets, which are more accurate than the 34-layer ones by considerable margins. Comparisons with state-of-the-art methods show that the 152-layer ResNet has a single-model top-5 validation error of 4.49%. The paper also examines the CIFAR-10 dataset, using simple architectures with 6nlayers with 3 3 convolutions and 2nlayers for each feature map size. The results show that extremely deep networks can be beneficial. This table summarizes the architecture of a deep learning model used to classify images on the CIFAR-10 dataset. The model consists of layers of 16, 32, and 64 filters, with shortcut connections connecting pairs of 3x3 layers. The model is trained with a mini-batch size of 128 on two GPUs, with a learning rate of 0.1, and data augmentation. The results show that the deep plain nets suffer from increased depth, but the ResNets manage to overcome the optimization difficulty and demonstrate accuracy gains when the depth increases. The 110-layer ResNet converges well and has fewer parameters than other deep and thin models. This paper explores the use of deep residual networks for image recognition tasks. It is shown that ResNets have generally smaller responses than their plain counterparts, which supports the basic motivation that the residual functions might be generally closer to zero than the non-residual functions. An aggressively deep model of over 1000 layers is explored, and it is shown that this model is able to achieve training error <0.1%. The testing result of this 1202-layer network is worse than that of the 110-layer network, which is attributed to overfitting. The paper also shows that the use of ResNet-101 for object detection on PASCAL VOC 2007 and 2012 and COCO leads to a 6.0% increase in COCO’s standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations. This paper introduces a detection method based on the Faster R-CNN system. The models are initialized by the ImageNet classiﬁcation models, and then ﬁne-tuned on the object detection data. The authors experiment with ResNet-50/101 and adopt the idea of “Networks on Conv feature maps” (NoC) to address the issue of no hidden fc layers. The full-image shared conv feature maps are computed using those layers whose strides on the image are no greater than 16 pixels. This method is used in the ILSVRC & COCO 2015 detection competitions and has shown to be successful in improving object detection accuracy. This paper discusses the use of ResNet-101 and VGG-16 for object detection. ResNet-101 is a deeper network than VGG-16 and is used to improve the features learned by the network. The paper also discusses the use of Batch Normalization layers, which are fixed during fine-tuning for object detection to reduce memory consumption. The paper then evaluates the performance of ResNet-101 and VGG-16 on the PASCAL VOC 2007 and 2012 datasets, and the MS COCO dataset. The results show that ResNet-101 improves the mAP by >3% over VGG-16 on the PASCAL VOC datasets, and by 6% on the MS COCO dataset. The paper also discusses improvements made for the competitions, such as box refinement and global context. This paper discusses the improvements made to the Faster R-CNN object detection system. The results of the single-scale training/testing show an improvement of 1 point in mAP@.5. Multi-scale training/testing has been developed to further improve the results. The multi-scale testing was performed only for the Fast R-CNN step, and the results show an improvement of 1.5 points in mAP@.5. The results of the multi-scale testing on the MS COCO dataset are presented in Table 9. The results of the multi-scale testing on the PASCAL VOC 2007 dataset are presented in Table 10 and Table 11. The results show an improvement of up to 13.2 points in mAP@.5. Overall, the multi-scale testing has improved the performance of the Faster R-CNN object detection system. This paper presents a Faster R-CNN system with improvements such as box refinement, context, and multi-scale testing. The system was tested on the PASCAL VOC 2012 test set and achieved an mAP@.5 of 55.7% and an mAP@[.5, .95] of 34.9%. The system was also tested on the ImageNet Detection (DET) task and achieved an mAP@.5 of 58.8% with a single model and 62.1% with an ensemble of 3 models. The system was also tested on the ImageNet Localization (LOC) task and achieved an error rate of 8.9% with an ensemble of 3 models. The results of this system surpassed the second place by 8.5 points in the ImageNet detection task in ILSVRC 2015. This paper presents a localization algorithm based on the RPN framework of Faster R-CNN with a few modifications. The algorithm is designed in a per-class form, with two sibling 1x1 convolutional layers for binary classification and box regression. The networks are pre-trained for ImageNet classification and then fine-tuned for localization. The algorithm is tested on the ImageNet dataset and compared to state-of-the-art methods. Results show that the algorithm significantly reduces the center-crop error to 13.3%, and with dense and multi-scale testing, the error is 11.7%. The R-CNN network is also used to update the proposals' scores and box positions.